{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fairscale.nn import PipeModule\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap your model with PipeModule\n",
    "model = PipeModule(model)\n",
    "\n",
    "# Define your training arguments\n",
    "training_args = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 10,\n",
    "    \"learning_rate\": 5e-5,\n",
    "}\n",
    "\n",
    "# Define your optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=training_args[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data loader\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=training_args[\"batch_size\"], shuffle=True\n",
    ")\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    eval_dataset, batch_size=training_args[\"batch_size\"], shuffle=False\n",
    ")\n",
    "\n",
    "# Wrap your model with DataParallel\n",
    "model = nn.DataParallel(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train your model\n",
    "for epoch in range(training_args[\"num_epochs\"]):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Loop over the training batches\n",
    "    for batch in train_loader:\n",
    "        # Move the batch to the device\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        # Get the inputs and labels\n",
    "        inputs = tokenizer(batch[\"sentence1\"], batch[\"sentence2\"], return_tensors=\"pt\", padding=True)\n",
    "        labels = batch[\"label\"]\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        # Backward pass and update the parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Loop over the evaluation batches\n",
    "    for batch in eval_loader:\n",
    "        # Move the batch to the device\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        # Get the inputs and labels\n",
    "        inputs = tokenizer(batch[\"sentence1\"], batch[\"sentence2\"], return_tensors=\"pt\", padding=True)\n",
    "        labels = batch[\"label\"]\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        # Compute the loss and accuracy\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "    # Print the results for this epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}, Accuracy: {acc.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
