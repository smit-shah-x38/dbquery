{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT = \"distilbert-base-uncased\"\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load IMDB data (training and eval)\"\"\"\n",
    "    raw_datasets = load_dataset(\"imdb\")\n",
    "    raw_datasets = raw_datasets.shuffle(seed=42)\n",
    "    # remove unnecessary data split\n",
    "    del raw_datasets[\"unsupervised\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "    # We will take a small sample in order to reduce the compute time, this is optional\n",
    "    train_population = random.sample(range(len(raw_datasets[\"train\"])), 100)\n",
    "    test_population = random.sample(range(len(raw_datasets[\"test\"])), 100)\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "    tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].select(train_population)\n",
    "    tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"].select(test_population)\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns(\"text\")\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    trainloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        shuffle=True,\n",
    "        batch_size=32,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n",
    "    )\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /root/anaconda3/envs/venv did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 13:01:48,545] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load as load_metric\n",
    "from transformers import AdamW\n",
    "\n",
    "def train(net, trainloader, epochs):\n",
    "    optimizer = AdamW(net.parameters(), lr=5e-5)\n",
    "    net.train()\n",
    "    for _ in range(epochs):\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            outputs = net(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "def test(net, testloader):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    loss = 0\n",
    "    net.eval()\n",
    "    for batch in testloader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = net(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss += outputs.loss.item()\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = metric.compute()[\"accuracy\"]\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "net = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CHECKPOINT, num_labels=2\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import flwr as fl\n",
    "\n",
    "class IMDBClient(fl.client.NumPyClient):\n",
    "        def get_parameters(self, config):\n",
    "            return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "        def set_parameters(self, parameters):\n",
    "            params_dict = zip(net.state_dict().keys(), parameters)\n",
    "            state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "            net.load_state_dict(state_dict, strict=True)\n",
    "        def fit(self, parameters, config):\n",
    "            self.set_parameters(parameters)\n",
    "            print(\"Training Started...\")\n",
    "            train(net, trainloader, epochs=1)\n",
    "            print(\"Training Finished.\")\n",
    "            return self.get_parameters(config={}), len(trainloader), {}\n",
    "        def evaluate(self, parameters, config):\n",
    "            self.set_parameters(parameters)\n",
    "            loss, accuracy = test(net, testloader)\n",
    "            return float(loss), len(testloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-07-31 13:01:52,247 | app.py:148 | Starting Flower server, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "INFO flwr 2023-07-31 13:01:52,258 | app.py:168 | Flower ECE: gRPC server running (3 rounds), SSL is disabled\n",
      "INFO flwr 2023-07-31 13:01:52,258 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-07-31 13:01:52,259 | server.py:273 | Requesting initial parameters from one random client\n"
     ]
    }
   ],
   "source": [
    "def weighted_average(metrics):\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    losses = [num_examples * m[\"loss\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples), \"loss\": sum(losses) / sum(examples)}\n",
    "\n",
    "# Define strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,\n",
    "    fraction_evaluate=1.0,\n",
    "    evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")\n",
    "\n",
    "# Start server\n",
    "fl.server.start_server(\n",
    "    server_address=\"0.0.0.0:8080\",\n",
    "    config=fl.server.ServerConfig(num_rounds=3),\n",
    "    strategy=strategy,\n",
    ")\n",
    "\n",
    "fl.client.start_numpy_client(\n",
    "    server_address=\"127.0.0.1:8080\",\n",
    "    client=IMDBClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
